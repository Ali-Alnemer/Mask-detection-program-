{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e475cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b4bce",
   "metadata": {},
   "source": [
    "# Data set cleaning\n",
    "1) Cropping the faces from the data  \n",
    "2) Regenerate the data with faces only  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7e31c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7c905e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_image_if_2_eyes(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        if len(eyes) >= 2:\n",
    "            return roi_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb927221",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/\"\n",
    "path_to_cr_data = \"./data/cropped/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "657dd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_dirs = []\n",
    "for entry in os.scandir(path_to_data):\n",
    "    if entry.is_dir():\n",
    "        img_dirs.append(entry.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a57f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train/cropped', './data/train/without_mask', './data/train/with_mask']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2350e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if os.path.exists(path_to_cr_data):\n",
    "     shutil.rmtree(path_to_cr_data)\n",
    "os.mkdir(path_to_cr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropped\n",
      "without_mask\n",
      "Generating cropped images in folder:  ./data/train/cropped/without_mask\n",
      "with_mask\n",
      "Generating cropped images in folder:  ./data/train/cropped/with_mask\n"
     ]
    }
   ],
   "source": [
    "cropped_image_dirs = []\n",
    "file_names_dict = {}\n",
    "\n",
    "for img_dir in img_dirs:\n",
    "    count = 1\n",
    "    name = img_dir.split('/')[-1]\n",
    "    print(name)\n",
    "    \n",
    "    file_names_dict[name] = []\n",
    "    \n",
    "    for entry in os.scandir(img_dir):\n",
    "        roi_color = get_cropped_image_if_2_eyes(entry.path)\n",
    "        if roi_color is not None:\n",
    "            cropped_folder = path_to_cr_data + name\n",
    "            if not os.path.exists(cropped_folder):\n",
    "                os.makedirs(cropped_folder)\n",
    "                cropped_image_dirs.append(cropped_folder)\n",
    "                print(\"Generating cropped images in folder: \",cropped_folder)\n",
    "                \n",
    "            cropped_file_name = name + str(count) + \".png\"\n",
    "            cropped_file_path = cropped_folder + \"/\" + cropped_file_name \n",
    "            \n",
    "            cv2.imwrite(cropped_file_path, roi_color)\n",
    "            file_names_dict[name].append(cropped_file_path)\n",
    "            count += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e2420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d08e8ded",
   "metadata": {},
   "source": [
    "# Image processing and labelling\n",
    "1) Load the data\n",
    "\n",
    "2) Save the data   \n",
    "\n",
    "3) Image processing\n",
    "\n",
    "4) Label the data\n",
    "\n",
    "5) Split train and test data\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9257035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda1e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29322/29322 [56:47<00:00,  8.60it/s]   \n",
      "100%|██████████| 33814/33814 [06:42<00:00, 84.03it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "CATEGORIES = [\"with_mask\", \"without_mask\"]\n",
    "data = []\n",
    "for category in CATEGORIES:  \n",
    "\n",
    "        path = os.path.join(DATADIR,category)  \n",
    "        class_num = CATEGORIES.index(category)  \n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  \n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  \n",
    "                data.append([new_array, class_num])  \n",
    "            except Exception as e:  \n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a66a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data1'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(data,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bda73920",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('data1','rb')\n",
    "data = pickle.load(infile)\n",
    "infile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c28c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ac9f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63136, 100, 100, 1)\n",
      "(63136,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for features,label in data:\n",
    "    X.append(features)\n",
    "    Y.append(label)\n",
    "\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "Y = np.array(Y)\n",
    "X = X / 255.0\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69be42e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a88faa",
   "metadata": {},
   "source": [
    "# Model training and testing  \n",
    "\n",
    "1) Trane the convolutional neural network \n",
    "\n",
    "2) Test the convolutional neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7675a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e45c6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 124)               1240124   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 124)               15500     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 1,255,749\n",
      "Trainable params: 1,255,749\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),\n",
    "    tf.keras.layers.Dense(124, activation='tanh'),\n",
    "    tf.keras.layers.Dense(124, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1 ,activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8ba93f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.RMSprop(lr=0.00001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "753a893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.2398 - accuracy: 0.9303\n",
      "Epoch 2/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.1086 - accuracy: 0.96711s - loss: 0\n",
      "Epoch 3/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0863 - accuracy: 0.9733\n",
      "Epoch 4/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0733 - accuracy: 0.9764\n",
      "Epoch 5/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0656 - accuracy: 0.9796\n",
      "Epoch 6/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0604 - accuracy: 0.9812\n",
      "Epoch 7/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0564 - accuracy: 0.9830\n",
      "Epoch 8/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0535 - accuracy: 0.9838\n",
      "Epoch 9/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0514 - accuracy: 0.9846\n",
      "Epoch 10/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0477 - accuracy: 0.9855\n",
      "Epoch 11/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0470 - accuracy: 0.9860\n",
      "Epoch 12/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0451 - accuracy: 0.9864\n",
      "Epoch 13/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0434 - accuracy: 0.9871\n",
      "Epoch 14/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0423 - accuracy: 0.9874\n",
      "Epoch 15/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0413 - accuracy: 0.9878\n",
      "Epoch 16/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0398 - accuracy: 0.9880\n",
      "Epoch 17/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0388 - accuracy: 0.98840s - loss: 0.039\n",
      "Epoch 18/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0385 - accuracy: 0.9883\n",
      "Epoch 19/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0371 - accuracy: 0.9891\n",
      "Epoch 20/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0360 - accuracy: 0.9894\n",
      "Epoch 21/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0354 - accuracy: 0.9893\n",
      "Epoch 22/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0343 - accuracy: 0.9900\n",
      "Epoch 23/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0336 - accuracy: 0.9898\n",
      "Epoch 24/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0329 - accuracy: 0.9903\n",
      "Epoch 25/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0320 - accuracy: 0.9909\n",
      "Epoch 26/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0316 - accuracy: 0.9911\n",
      "Epoch 27/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0307 - accuracy: 0.9910\n",
      "Epoch 28/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0308 - accuracy: 0.9908\n",
      "Epoch 29/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0291 - accuracy: 0.9914\n",
      "Epoch 30/100\n",
      "1579/1579 [==============================] - 25s 16ms/step - loss: 0.0294 - accuracy: 0.9913\n",
      "Epoch 31/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0283 - accuracy: 0.9919\n",
      "Epoch 32/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0283 - accuracy: 0.9920\n",
      "Epoch 33/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0276 - accuracy: 0.9921\n",
      "Epoch 34/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0270 - accuracy: 0.9925\n",
      "Epoch 35/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0265 - accuracy: 0.9924\n",
      "Epoch 36/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0261 - accuracy: 0.9925\n",
      "Epoch 37/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0260 - accuracy: 0.9923\n",
      "Epoch 38/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0251 - accuracy: 0.9929\n",
      "Epoch 39/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0251 - accuracy: 0.9927\n",
      "Epoch 40/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0241 - accuracy: 0.9931\n",
      "Epoch 41/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0241 - accuracy: 0.9928\n",
      "Epoch 42/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0235 - accuracy: 0.9933\n",
      "Epoch 43/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0230 - accuracy: 0.9931\n",
      "Epoch 44/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0224 - accuracy: 0.9936\n",
      "Epoch 45/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0223 - accuracy: 0.9940\n",
      "Epoch 46/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0219 - accuracy: 0.9935\n",
      "Epoch 47/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0219 - accuracy: 0.9936\n",
      "Epoch 48/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0211 - accuracy: 0.9939\n",
      "Epoch 49/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0211 - accuracy: 0.9938\n",
      "Epoch 50/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0203 - accuracy: 0.9939\n",
      "Epoch 51/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0201 - accuracy: 0.9944\n",
      "Epoch 52/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0196 - accuracy: 0.9944\n",
      "Epoch 53/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0206 - accuracy: 0.9935\n",
      "Epoch 54/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0192 - accuracy: 0.9946\n",
      "Epoch 55/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0187 - accuracy: 0.9946\n",
      "Epoch 56/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0189 - accuracy: 0.9948\n",
      "Epoch 57/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0183 - accuracy: 0.9949\n",
      "Epoch 58/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0185 - accuracy: 0.9947\n",
      "Epoch 59/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0176 - accuracy: 0.9950\n",
      "Epoch 60/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0174 - accuracy: 0.9950\n",
      "Epoch 61/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0175 - accuracy: 0.9952\n",
      "Epoch 62/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0167 - accuracy: 0.9954\n",
      "Epoch 63/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0167 - accuracy: 0.9954\n",
      "Epoch 64/100\n",
      "1579/1579 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.99 - 24s 15ms/step - loss: 0.0173 - accuracy: 0.9949\n",
      "Epoch 65/100\n",
      "1579/1579 [==============================] - 25s 16ms/step - loss: 0.0164 - accuracy: 0.9956\n",
      "Epoch 66/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0159 - accuracy: 0.9959\n",
      "Epoch 67/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0162 - accuracy: 0.9956\n",
      "Epoch 68/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0156 - accuracy: 0.9957\n",
      "Epoch 69/100\n",
      "1579/1579 [==============================] - 25s 16ms/step - loss: 0.0155 - accuracy: 0.9957\n",
      "Epoch 70/100\n",
      "1579/1579 [==============================] - 25s 16ms/step - loss: 0.0154 - accuracy: 0.99590s - loss: 0.0156 \n",
      "Epoch 71/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0151 - accuracy: 0.9958\n",
      "Epoch 72/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0151 - accuracy: 0.9958\n",
      "Epoch 73/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 74/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0145 - accuracy: 0.9959\n",
      "Epoch 75/100\n",
      "1579/1579 [==============================] - 25s 16ms/step - loss: 0.0147 - accuracy: 0.9960\n",
      "Epoch 76/100\n",
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0142 - accuracy: 0.9961\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579/1579 [==============================] - 24s 15ms/step - loss: 0.0144 - accuracy: 0.9961\n",
      "Epoch 78/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0141 - accuracy: 0.9960\n",
      "Epoch 79/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0138 - accuracy: 0.9965\n",
      "Epoch 80/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0132 - accuracy: 0.9966\n",
      "Epoch 81/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0139 - accuracy: 0.9962\n",
      "Epoch 82/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0135 - accuracy: 0.9967\n",
      "Epoch 83/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0135 - accuracy: 0.9961\n",
      "Epoch 84/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0129 - accuracy: 0.9966\n",
      "Epoch 85/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0127 - accuracy: 0.9968\n",
      "Epoch 86/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0129 - accuracy: 0.9966\n",
      "Epoch 87/100\n",
      "1579/1579 [==============================] - 22s 14ms/step - loss: 0.0126 - accuracy: 0.9966\n",
      "Epoch 88/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0126 - accuracy: 0.9966\n",
      "Epoch 89/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0124 - accuracy: 0.9968\n",
      "Epoch 90/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0123 - accuracy: 0.9969\n",
      "Epoch 91/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0122 - accuracy: 0.9968\n",
      "Epoch 92/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0121 - accuracy: 0.9970\n",
      "Epoch 93/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 94/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0119 - accuracy: 0.9970\n",
      "Epoch 95/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0114 - accuracy: 0.9971\n",
      "Epoch 96/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0111 - accuracy: 0.9972\n",
      "Epoch 97/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0116 - accuracy: 0.9971\n",
      "Epoch 98/100\n",
      "1579/1579 [==============================] - 23s 15ms/step - loss: 0.0114 - accuracy: 0.9968\n",
      "Epoch 99/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 100/100\n",
      "1579/1579 [==============================] - 23s 14ms/step - loss: 0.0109 - accuracy: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1480a6d8bb0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "740c16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 - 2s - loss: 0.0234 - accuracy: 0.9946\n",
      "\n",
      "Test accuracy: 0.9946151375770569\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(xtest,  ytest, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8d7ba3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mask model 9\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048037d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5957a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f185a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac610edb",
   "metadata": {},
   "source": [
    "# Life cam and prediction\n",
    "\n",
    "1) Recognising face using life camera\n",
    "\n",
    "2) Predict if the face wearing a mask or not \n",
    "\n",
    "3) Show the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a889455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2fd67b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('mask model 9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22bd51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,3)  \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(IMG_SIZE,IMG_SIZE))\n",
    "        normalized=resized/255\n",
    "        reshaped=np.reshape(normalized,(-1,IMG_SIZE,IMG_SIZE,1))\n",
    "        ar = np.expand_dims(reshaped, axis = 0)\n",
    "        result=model.predict(ar)\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "        if  result <=0.999:\n",
    "            label=0\n",
    "            \n",
    "        else:\n",
    "            label=1\n",
    "        \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e2668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c61d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6477473c",
   "metadata": {},
   "source": [
    "# Life cam prediction and gate controlling \n",
    "\n",
    "1) Recognising face using life camera\n",
    "\n",
    "2) Predict if the face wearing a mask or not \n",
    "\n",
    "3) Show the result\n",
    "\n",
    "4) Open and close the gait depending on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fee8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfirmata import Arduino, SERVO,util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ef0df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin=10\n",
    "board=Arduino('COM3')\n",
    "\n",
    "board.digital[pin].mode=SERVO\n",
    "\n",
    "\n",
    "    \n",
    "def gatard(x):\n",
    "    if x==0:\n",
    "        board.digital[10].write(90)\n",
    "       \n",
    "    elif x==1:\n",
    "        board.digital[10].write(0)\n",
    "  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34fbfbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('mask model 9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "606b6cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-u4kjpz2z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-38b3a76619d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mfaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_clsfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-u4kjpz2z\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "while(True):\n",
    "\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "    \n",
    "        face_img=gray[y:y+w,x:x+w]\n",
    "        resized=cv2.resize(face_img,(IMG_SIZE,IMG_SIZE))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(-1,IMG_SIZE,IMG_SIZE,1))\n",
    "        ar = np.expand_dims(reshaped, axis = 0)\n",
    "        result=model.predict(ar)\n",
    "\n",
    "        \n",
    "     \n",
    "        \n",
    "        if  result <= 0.98:\n",
    "            label=0\n",
    "            \n",
    "        else:\n",
    "            label=1\n",
    "        \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "        gatard(label)\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    "    if(key==27):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb77c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
